{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 資料預處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from C:\\Users\\User\\pythonRepository\\SemanticAnalysis\\jieba_dict\\dict.txt.big ...\n",
      "Loading model from cache C:\\Users\\User\\AppData\\Local\\Temp\\jieba.ua442fad0dbeebb90d0fa2ffe39e75bc0.cache\n",
      "Loading model cost 1.197 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['咳嗽 ', '喉嚨 ', '喉嚨痛 ', '喉嚨 不 舒服 ', '喉嚨 癢 ', '喉嚨 發炎 ', '喉嚨 ', '喉嚨 乾 ', '喉嚨 腫 ', '咳嗽 ', '喉嚨 ', '喉嚨痛 ', '喉嚨 不 舒服 ', '喉嚨 癢 ', '喉嚨 發炎 ', '喉嚨 ', '喉嚨 乾 ', '喉嚨 腫 ', '咳嗽 ', '喉嚨 ', '喉嚨痛 ', '喉嚨 不 舒服 ', '喉嚨 癢 ', '喉嚨 發炎 ', '喉嚨 ', '喉嚨 乾 ', '喉嚨 腫 ', '火 大 ', '火氣 大 ', '火 很大 ', '很火 ', '煩躁 ', '很煩 ', '厭煩 ', '火 大 ', '火氣 大 ', '火 很大 ', '很火 ', '煩躁 ', '很煩 ', '厭煩 ', '火 大 ', '火氣 大 ', '火 很大 ', '很火 ', '煩躁 ', '很煩 ', '厭煩 ', '腹痛 ', '肚子痛 ', '肚子 不 舒服 ', '肚子 怪怪的 ', '肚子疼 ', '肚子 脹 ', '腹部 痛 ', '腹痛 ', '肚子痛 ', '肚子 不 舒服 ', '肚子 怪怪的 ', '肚子疼 ', '肚子 脹 ', '腹部 痛 ', '腹痛 ', '肚子痛 ', '肚子 不 舒服 ', '肚子 怪怪的 ', '肚子疼 ', '肚子 脹 ', '腹部 痛']\n",
      "['0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2']\n"
     ]
    }
   ],
   "source": [
    "# 讀資料\n",
    "import jieba\n",
    "\n",
    "text = []\n",
    "mark = []\n",
    "\n",
    "# set jieba dict\n",
    "jieba.set_dictionary('jieba_dict/dict.txt.big')\n",
    "\n",
    "# 讀text資料\n",
    "with open('symptom_words/text.txt', 'r') as label_text:\n",
    "    for line in label_text:\n",
    "        seg_line = jieba.cut(line, cut_all=False)\n",
    "        text.append(' '.join(seg_line).rstrip('\\n'))\n",
    "        \n",
    "# 讀mark資料\n",
    "with open('symptom_words/mark.txt', 'r') as feacture_text:\n",
    "    for line in feacture_text:\n",
    "        mark.append(line.rstrip('\\n'))        \n",
    "        \n",
    "print(text)\n",
    "print(mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 建立token\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    }
   ],
   "source": [
    "print(token.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'喉嚨': 1, '肚子': 2, '不': 3, '舒服': 4, '火': 5, '大': 6, '咳嗽': 7, '喉嚨痛': 8, '癢': 9, '發炎': 10, '乾': 11, '腫': 12, '火氣': 13, '很大': 14, '很火': 15, '煩躁': 16, '很煩': 17, '厭煩': 18, '腹痛': 19, '肚子痛': 20, '怪怪的': 21, '肚子疼': 22, '脹': 23, '腹部': 24, '痛': 25}\n"
     ]
    }
   ],
   "source": [
    "print(token.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 把text轉為數字list\n",
    "text_seq = token.texts_to_sequences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7], [1], [8], [1, 3, 4], [1, 9], [1, 10], [1], [1, 11], [1, 12], [7], [1], [8], [1, 3, 4], [1, 9], [1, 10], [1], [1, 11], [1, 12], [7], [1], [8], [1, 3, 4], [1, 9], [1, 10], [1], [1, 11], [1, 12], [5, 6], [13, 6], [5, 14], [15], [16], [17], [18], [5, 6], [13, 6], [5, 14], [15], [16], [17], [18], [5, 6], [13, 6], [5, 14], [15], [16], [17], [18], [19], [20], [2, 3, 4], [2, 21], [22], [2, 23], [24, 25], [19], [20], [2, 3, 4], [2, 21], [22], [2, 23], [24, 25], [19], [20], [2, 3, 4], [2, 21], [22], [2, 23], [24, 25]]\n"
     ]
    }
   ],
   "source": [
    "print(text_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 讓每筆list長度都是4\n",
    "train_text = sequence.pad_sequences(text_seq, maxlen=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  7]\n",
      " [ 0  0  0  1]\n",
      " [ 0  0  0  8]\n",
      " [ 0  1  3  4]\n",
      " [ 0  0  1  9]\n",
      " [ 0  0  1 10]\n",
      " [ 0  0  0  1]\n",
      " [ 0  0  1 11]\n",
      " [ 0  0  1 12]\n",
      " [ 0  0  0  7]\n",
      " [ 0  0  0  1]\n",
      " [ 0  0  0  8]\n",
      " [ 0  1  3  4]\n",
      " [ 0  0  1  9]\n",
      " [ 0  0  1 10]\n",
      " [ 0  0  0  1]\n",
      " [ 0  0  1 11]\n",
      " [ 0  0  1 12]\n",
      " [ 0  0  0  7]\n",
      " [ 0  0  0  1]\n",
      " [ 0  0  0  8]\n",
      " [ 0  1  3  4]\n",
      " [ 0  0  1  9]\n",
      " [ 0  0  1 10]\n",
      " [ 0  0  0  1]\n",
      " [ 0  0  1 11]\n",
      " [ 0  0  1 12]\n",
      " [ 0  0  5  6]\n",
      " [ 0  0 13  6]\n",
      " [ 0  0  5 14]\n",
      " [ 0  0  0 15]\n",
      " [ 0  0  0 16]\n",
      " [ 0  0  0 17]\n",
      " [ 0  0  0 18]\n",
      " [ 0  0  5  6]\n",
      " [ 0  0 13  6]\n",
      " [ 0  0  5 14]\n",
      " [ 0  0  0 15]\n",
      " [ 0  0  0 16]\n",
      " [ 0  0  0 17]\n",
      " [ 0  0  0 18]\n",
      " [ 0  0  5  6]\n",
      " [ 0  0 13  6]\n",
      " [ 0  0  5 14]\n",
      " [ 0  0  0 15]\n",
      " [ 0  0  0 16]\n",
      " [ 0  0  0 17]\n",
      " [ 0  0  0 18]\n",
      " [ 0  0  0 19]\n",
      " [ 0  0  0 20]\n",
      " [ 0  2  3  4]\n",
      " [ 0  0  2 21]\n",
      " [ 0  0  0 22]\n",
      " [ 0  0  2 23]\n",
      " [ 0  0 24 25]\n",
      " [ 0  0  0 19]\n",
      " [ 0  0  0 20]\n",
      " [ 0  2  3  4]\n",
      " [ 0  0  2 21]\n",
      " [ 0  0  0 22]\n",
      " [ 0  0  2 23]\n",
      " [ 0  0 24 25]\n",
      " [ 0  0  0 19]\n",
      " [ 0  0  0 20]\n",
      " [ 0  2  3  4]\n",
      " [ 0  0  2 21]\n",
      " [ 0  0  0 22]\n",
      " [ 0  0  2 23]\n",
      " [ 0  0 24 25]]\n"
     ]
    }
   ],
   "source": [
    "print(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one-hot encoding mark\n",
    "from keras.utils import np_utils\n",
    "\n",
    "mark_oneHot = np_utils.to_categorical(mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  1.  0.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "print(mark_oneHot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建模&訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(output_dim=4, input_dim=26, input_length=4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Dense(units=3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 4)              104       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               4352      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 5,227\n",
      "Trainable params: 5,227\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 62 samples, validate on 7 samples\n",
      "Epoch 1/30\n",
      "0s - loss: 0.6333 - acc: 0.6667 - val_loss: 0.6493 - val_acc: 0.6667\n",
      "Epoch 2/30\n",
      "0s - loss: 0.6259 - acc: 0.6667 - val_loss: 0.6566 - val_acc: 0.6667\n",
      "Epoch 3/30\n",
      "0s - loss: 0.6195 - acc: 0.6667 - val_loss: 0.6659 - val_acc: 0.6667\n",
      "Epoch 4/30\n",
      "0s - loss: 0.6112 - acc: 0.6667 - val_loss: 0.6694 - val_acc: 0.6667\n",
      "Epoch 5/30\n",
      "0s - loss: 0.6015 - acc: 0.6667 - val_loss: 0.6667 - val_acc: 0.6667\n",
      "Epoch 6/30\n",
      "0s - loss: 0.5897 - acc: 0.6667 - val_loss: 0.6668 - val_acc: 0.6667\n",
      "Epoch 7/30\n",
      "0s - loss: 0.5731 - acc: 0.6667 - val_loss: 0.6613 - val_acc: 0.6667\n",
      "Epoch 8/30\n",
      "0s - loss: 0.5540 - acc: 0.7097 - val_loss: 0.6489 - val_acc: 0.6667\n",
      "Epoch 9/30\n",
      "0s - loss: 0.5300 - acc: 0.7312 - val_loss: 0.6271 - val_acc: 0.6667\n",
      "Epoch 10/30\n",
      "0s - loss: 0.5015 - acc: 0.7742 - val_loss: 0.6238 - val_acc: 0.6667\n",
      "Epoch 11/30\n",
      "0s - loss: 0.4689 - acc: 0.7796 - val_loss: 0.6188 - val_acc: 0.6667\n",
      "Epoch 12/30\n",
      "0s - loss: 0.4328 - acc: 0.7957 - val_loss: 0.6024 - val_acc: 0.6667\n",
      "Epoch 13/30\n",
      "0s - loss: 0.3934 - acc: 0.8226 - val_loss: 0.5582 - val_acc: 0.7619\n",
      "Epoch 14/30\n",
      "0s - loss: 0.3528 - acc: 0.9140 - val_loss: 0.5019 - val_acc: 0.7619\n",
      "Epoch 15/30\n",
      "0s - loss: 0.3128 - acc: 0.9301 - val_loss: 0.4645 - val_acc: 0.8095\n",
      "Epoch 16/30\n",
      "0s - loss: 0.2769 - acc: 0.9570 - val_loss: 0.4112 - val_acc: 0.8095\n",
      "Epoch 17/30\n",
      "0s - loss: 0.2387 - acc: 0.9677 - val_loss: 0.3526 - val_acc: 0.8571\n",
      "Epoch 18/30\n",
      "0s - loss: 0.2047 - acc: 0.9731 - val_loss: 0.2962 - val_acc: 0.9524\n",
      "Epoch 19/30\n",
      "0s - loss: 0.1752 - acc: 0.9892 - val_loss: 0.2463 - val_acc: 1.0000\n",
      "Epoch 20/30\n",
      "0s - loss: 0.1461 - acc: 1.0000 - val_loss: 0.2245 - val_acc: 1.0000\n",
      "Epoch 21/30\n",
      "0s - loss: 0.1210 - acc: 1.0000 - val_loss: 0.1833 - val_acc: 1.0000\n",
      "Epoch 22/30\n",
      "0s - loss: 0.1019 - acc: 1.0000 - val_loss: 0.1485 - val_acc: 1.0000\n",
      "Epoch 23/30\n",
      "0s - loss: 0.0830 - acc: 1.0000 - val_loss: 0.1249 - val_acc: 1.0000\n",
      "Epoch 24/30\n",
      "0s - loss: 0.0680 - acc: 1.0000 - val_loss: 0.1056 - val_acc: 1.0000\n",
      "Epoch 25/30\n",
      "0s - loss: 0.0564 - acc: 1.0000 - val_loss: 0.0909 - val_acc: 1.0000\n",
      "Epoch 26/30\n",
      "0s - loss: 0.0468 - acc: 1.0000 - val_loss: 0.0764 - val_acc: 1.0000\n",
      "Epoch 27/30\n",
      "0s - loss: 0.0392 - acc: 1.0000 - val_loss: 0.0592 - val_acc: 1.0000\n",
      "Epoch 28/30\n",
      "0s - loss: 0.0331 - acc: 1.0000 - val_loss: 0.0493 - val_acc: 1.0000\n",
      "Epoch 29/30\n",
      "0s - loss: 0.0281 - acc: 1.0000 - val_loss: 0.0415 - val_acc: 1.0000\n",
      "Epoch 30/30\n",
      "0s - loss: 0.0241 - acc: 1.0000 - val_loss: 0.0348 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 定義訓練方式\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 開始訓練\n",
    "# 打散資料:shuffle=True,\n",
    "train_history = model.fit(train_text, mark_oneHot, batch_size=10, epochs=30, verbose=2, shuffle=True, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "32/69 [============>.................] - ETA: 0s"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 評估\n",
    "scores = model.evaluate(train_text, mark_oneHot)\n",
    "scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "輸入要查詢的字詞：肚子痛\n",
      "['肚子痛']\n",
      "[[20]]\n",
      "1/1 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "query = input('輸入要查詢的字詞：')\n",
    "seg_query = jieba.cut(query, cut_all=False)\n",
    "query_line = []\n",
    "query_line.append(' '.join(seg_query).rstrip('\\n'))\n",
    "print(query_line)\n",
    "\n",
    "input_seq = token.texts_to_sequences(query_line) # 轉成數字list\n",
    "print(input_seq)\n",
    "\n",
    "pad_input_seq = sequence.pad_sequences(input_seq, maxlen=4) # 長度轉為4\n",
    "\n",
    "predict_soft = model.predict(pad_input_seq)\n",
    "predict = model.predict_classes(pad_input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0:咳嗽, 1:火大, 2:腹痛\n",
    "predict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.02073482  0.04021065  0.93905455]]\n"
     ]
    }
   ],
   "source": [
    "print(predict_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
